{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Encoder\n",
    "\n",
    "Reads the file `test.flac` and compresses it.\n",
    "The result will be two files: one containing the weights of the model: `weights.h5` and  the other containing the compressed representation `compressed.npy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "lL3MdajrN_vc",
    "outputId": "0d18010b-ece4-427d-de8d-66eb751089bd"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.losses as klosses\n",
    "import tensorflow.keras.layers as layers\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "numpy.random.seed(4)\n",
    "random.seed(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VI-ITKqS5TTP"
   },
   "outputs": [],
   "source": [
    "def create_encoder_decoder(data_shape, compression_factor, activation_en='elu', activation_de='elu', optimizer='adam', kernel_width_en=256, kernel_width_de=64, channel_count_en=16, channel_count_de=16, regularizer=None):\n",
    "  \"\"\"creates an encoder-decoder pair to be used in an autoencoder\"\"\"\n",
    "  acfun_in = activation_en\n",
    "  acfun_out = activation_de\n",
    "  layer_count = compression_exponent(compression_factor)\n",
    "\n",
    "  encoder = keras.Sequential(name = \"encoder\")\n",
    "  encoder.add(layers.Conv2D(channel_count_en, (kernel_width_en, data_shape[1]), activation='linear', padding='same', kernel_regularizer=regularizer, input_shape=data_shape))\n",
    "  for _ in range(layer_count // 2):\n",
    "    encoder.add(layers.Conv2D(channel_count_en, (kernel_width_en, data_shape[1]), strides = (4, 1), activation=acfun_in, padding='same', kernel_regularizer=regularizer))\n",
    "  if layer_count % 2 == 1:\n",
    "    encoder.add(layers.Conv2D(channel_count_en, (kernel_width_en, data_shape[1]), strides = (8, 1), activation=acfun_in, padding='same', kernel_regularizer=regularizer))\n",
    "  else:\n",
    "    encoder.add(layers.Conv2D(channel_count_en, (kernel_width_en, data_shape[1]), strides = (4, 1), activation=acfun_in, padding='same', kernel_regularizer=regularizer))\n",
    "\n",
    "  encoder.add(layers.Conv2D(4, (kernel_width_en, data_shape[1]), activation = 'linear', padding='same'))\n",
    "  \n",
    "  decoder = keras.Sequential(name = \"decoder\")\n",
    "  if layer_count % 2 == 1:\n",
    "    decoder.add(layers.Conv2DTranspose(channel_count_de, (kernel_width_de, data_shape[1]), strides=(8, 1), activation=acfun_out, padding='same'))\n",
    "  else:\n",
    "    decoder.add(layers.Conv2DTranspose(channel_count_de, (kernel_width_de, data_shape[1]), strides=(4, 1), activation=acfun_out, padding='same'))\n",
    "  for _ in range(layer_count // 2):\n",
    "    decoder.add(layers.Conv2DTranspose(channel_count_de, (kernel_width_de, data_shape[1]), strides=(4, 1), activation=acfun_out, padding='same'))\n",
    "  decoder.add(layers.Conv2D(1, (kernel_width_de, data_shape[1]), activation='linear', padding='same'))\n",
    "\n",
    "  return (encoder, decoder)\n",
    "\n",
    "def create_model(data_shape, activation_en='elu', activation_de='elu', optimizer='adam', kernel_width_en=256, kernel_width_de=64, channel_count_en=16, channel_count_de=16, compression_factor=8, loss='mse', regularizer = None):\n",
    "  \"\"\"creates an autoencoder\"\"\"\n",
    "  enc_dec = create_encoder_decoder(data_shape, compression_factor, activation_en, activation_de, optimizer, kernel_width_en, kernel_width_de, channel_count_en, channel_count_de, regularizer)\n",
    "  model = keras.Sequential(name = \"autoencoder\")\n",
    "  model.add(enc_dec[0])\n",
    "  model.add(enc_dec[1])\n",
    "  model.compile(\n",
    "          loss=loss,\n",
    "          optimizer=optimizer,\n",
    "          metrics=[keras.metrics.MeanSquaredError()])\n",
    "  return (model, enc_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MYdB4PtpV8Xc"
   },
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "  \"\"\"normalizes the data and returns the parameters necessary to reconstruct the original\"\"\"\n",
    "  MIN = data.min()\n",
    "  data = data - MIN\n",
    "  DIV = data.max()\n",
    "  data = data / DIV\n",
    "  return (data, MIN, DIV)\n",
    "\n",
    "def denormalize(data_tuple):\n",
    "  \"\"\"reverts the normalization\"\"\"\n",
    "  (data, MIN, DIV) = data_tuple\n",
    "  return data * DIV + MIN\n",
    "\n",
    "def compression_exponent(factor):\n",
    "  \"\"\"returns the smalles n for 2^n > factor\"\"\"\n",
    "  return np.int_(np.ceil(np.log2(np.float64(factor))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9Ue8dXcuAEn"
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_data_for_training(song_data, compression_factor, window_width, stride, batch_size=8):\n",
    "  \"\"\"creates a generator, that returns batches of the training data\n",
    "  \n",
    "     returns the Generator and the number of steps per epoch\n",
    "  \"\"\"\n",
    "  comp_fac = 2 ** (compression_exponent(compression_factor) + 2)\n",
    "  window_width += (comp_fac - (window_width % comp_fac)) % comp_fac\n",
    "  segment_indices = [(i, i + window_width) for i in range(0, max(1, song_data.shape[0] - window_width), stride)]\n",
    "  def generator():\n",
    "    while True:\n",
    "      windows = map(lambda seg: song_data[seg[0] : seg[1]], segment_indices)\n",
    "      windows = map(lambda segment: np.concatenate([segment, np.zeros((window_width - segment.shape[0], segment.shape[1]))]), windows)\n",
    "      while True:\n",
    "        targets = []\n",
    "        for i, w in zip(range(batch_size), windows):\n",
    "          targets.append(w.reshape((1, window_width, song_data.shape[1], 1)))\n",
    "        if len(targets) > 0:\n",
    "          targets = np.concatenate(targets)\n",
    "          yield (targets, targets)\n",
    "        else:\n",
    "          break\n",
    "\n",
    "  return (generator(), int(np.ceil(len(segment_indices) / batch_size))) \n",
    "  \n",
    "def transform_data_for_model(song_data, compression_factor):\n",
    "  \"\"\"transforms data to have a compatible size fopr the model\"\"\"\n",
    "  comp_fac = 2 ** (compression_exponent(compression_factor) + 2)\n",
    "  padding_size = (comp_fac - (song_data.shape[0] % comp_fac)) % comp_fac\n",
    "  nd = np.concatenate([song_data, np.zeros((padding_size, song_data.shape[1]))])\n",
    "  nd = nd.reshape((1, nd.shape[0], nd.shape[1], 1))\n",
    "  return (nd, padding_size)\n",
    "\n",
    "def transform_data_from_model(model_data, padding):\n",
    "  \"\"\"reverts the transform_data_for_model function\"\"\"\n",
    "  data = model_data[0][:model_data.shape[1] - padding]\n",
    "  return data.reshape((data.shape[0], data.shape[1]))\n",
    "\n",
    "def predict(data, model, compression_factor, overlap = 2**16, segment_size = 2**18):\n",
    "  \"\"\"predicts the data the model\"\"\"\n",
    "  data_segments = [data[max(0, i - overlap):min(i + overlap + segment_size, data.shape[0])] for i in range(0, data.shape[0], segment_size)]\n",
    "  prepared_data = [transform_data_for_model(segment, compression_factor) for segment in data_segments]\n",
    "  model_input = [d[0] for d in prepared_data]\n",
    "  raw_prediction = [model.predict(i) for i in model_input]\n",
    "  prediction = [transform_data_from_model(tup[0], tup[1][1]) for tup in zip(raw_prediction, prepared_data)]\n",
    "  padding_free_prediction = [prediction[0][:segment_size]] + [pred[overlap : -overlap] for pred in prediction[1:-1]] + [prediction[-1][overlap:]]\n",
    "  return np.concatenate(padding_free_prediction)\n",
    "\n",
    "def evaluate(data, model, compression_factor, overlap = 2**16, segment_size = 2**18):\n",
    "  \"\"\"evaluates the model based on mse\"\"\"\n",
    "  pred = predict(data, model, compression_factor, overlap, segment_size)\n",
    "  return np.sum((pred - data) ** 2) / data.size\n",
    "\n",
    "def compress(data, compressor, compression_factor, overlap = 2 ** 16, segment_size = 2 ** 18):\n",
    "  \"\"\"Compress the given input data and returns the code\"\"\"  \n",
    "  data_segments = [data[max(0, i - overlap):min(i + overlap + segment_size, data.shape[0])] for i in range(0, data.shape[0], segment_size)]\n",
    "  prepared_data = [transform_data_for_model(segment, compression_factor) for segment in data_segments]\n",
    "  preds = [[compressor.predict(i[0]), np.int32(i[1])] for i in prepared_data]\n",
    "  return preds \n",
    "\n",
    "def decompress(data, decompressor, compression_factor, overlap = 2 ** 16, segment_size = 2 ** 18):\n",
    "  \"\"\"decompress the compressed data\"\"\"  \n",
    "  model_input = [d[0] for d in data]\n",
    "  raw_prediction = [decompressor.predict(i) for i in model_input]\n",
    "  prediction = [transform_data_from_model(tup[0], tup[1][1]) for tup in zip(raw_prediction, data)]\n",
    "  padding_free_prediction = [prediction[0][:segment_size]] + [pred[overlap : -overlap] for pred in prediction[1:-1]] + [prediction[-1][overlap:]]\n",
    "  return np.concatenate(padding_free_prediction) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9KYVAuxU8XDr"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PATH_PREFIX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-732d96fa932d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msong_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplerate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH_PREFIX\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"test2.flac\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnormalized_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDIV\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msong_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PATH_PREFIX' is not defined"
     ]
    }
   ],
   "source": [
    "song_data, samplerate = sf.read(\"input.flac\")\n",
    "(normalized_data, MIN, DIV) = normalize(song_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "ut2WBnzPO6kP",
    "outputId": "34b0fd21-8ac5-40e1-b122-9738c9cbd1ab"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normalized_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-7f06abcc7f55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m (model, enc_dec) = create_model((None, normalized_data.shape[1], 1),\n\u001b[0m\u001b[0;32m     14\u001b[0m                             \u001b[0mactivation_en\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivation_en\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                             \u001b[0mactivation_de\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivation_de\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'normalized_data' is not defined"
     ]
    }
   ],
   "source": [
    "compression_factor = 4\n",
    "\n",
    "activation_en = 'selu'\n",
    "activation_de = 'selu'\n",
    "channel_size_en = 16\n",
    "channel_size_de = 16\n",
    "kernel_size_en = 64\n",
    "kernel_size_de = 64\n",
    "optimizer = 'adam'\n",
    "loss = 'mae'\n",
    "\n",
    "\n",
    "(model, enc_dec) = create_model((None, normalized_data.shape[1], 1),\n",
    "                            activation_en = activation_en,\n",
    "                            activation_de = activation_de,\n",
    "                            optimizer = optimizer,\n",
    "                            kernel_width_en = kernel_size_en,\n",
    "                            kernel_width_de = kernel_size_de,\n",
    "                            channel_count_en = channel_size_en,\n",
    "                            channel_count_de = channel_size_de,\n",
    "                            compression_factor = compression_factor,\n",
    "                            loss = loss)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "V1XAHwKe8WUr",
    "outputId": "84eacade-0ee5-4240-cbd2-6eadf0328180"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normalized_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-f26040bf2bc4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_generator_small\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch_small\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_data_for_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormalized_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression_factor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m18\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata_generator_big\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch_big\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_data_for_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormalized_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression_factor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m18\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mhistory_small\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_generator_small\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msteps_per_epoch_small\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mhistory_big\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_generator_big\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msteps_per_epoch_big\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m48\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'normalized_data' is not defined"
     ]
    }
   ],
   "source": [
    "data_generator_small, steps_per_epoch_small = generate_data_for_training(normalized_data, compression_factor, 2 ** 18, 2 ** 16, 4)\n",
    "data_generator_big, steps_per_epoch_big = generate_data_for_training(normalized_data, compression_factor, 2 ** 20, 2 ** 18, 4)\n",
    "\n",
    "history_small = model.fit_generator(data_generator_small, steps_per_epoch = steps_per_epoch_small, epochs = 16)\n",
    "history_big = model.fit_generator(data_generator_big, steps_per_epoch = steps_per_epoch_big, epochs = 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KHdwOlhUSA1P"
   },
   "outputs": [],
   "source": [
    "enc_dec[1].save_weights('weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0jBawAHrY3Og"
   },
   "outputs": [],
   "source": [
    "compressed = compress(normalized_data, enc_dec[0], compression_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EsOR6_FuZdtG"
   },
   "outputs": [],
   "source": [
    "saveable_compressed = np.asarray([compressed, MIN, DIV, samplerate])\n",
    "np.save('compressed.npy', saveable_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cd-3hJmekNmb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Compressor.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
