{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Decoder\n",
    "\n",
    "It take the output files of the Compressor and tries to reconstruct the original File."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "lL3MdajrN_vc",
    "outputId": "0495ab94-6cd7-4e92-d737-7796dbd0aef0"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.losses as klosses\n",
    "import tensorflow.keras.layers as layers\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "numpy.random.seed(4)\n",
    "random.seed(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VI-ITKqS5TTP"
   },
   "outputs": [],
   "source": [
    "def create_encoder_decoder(data_shape, compression_factor = 8, activation_en='elu', activation_de='elu', optimizer='adam', kernel_width_en=256, kernel_width_de=64, channel_count_en=16, channel_count_de=16, regularizer=None):\n",
    "  \"\"\"creates an encoder-decoder pair to be used in an autoencoder\"\"\"\n",
    "  acfun_in = activation_en\n",
    "  acfun_out = activation_de\n",
    "  layer_count = compression_exponent(compression_factor)\n",
    "\n",
    "  encoder = keras.Sequential(name = \"encoder\")\n",
    "  encoder.add(layers.Conv2D(channel_count_en, (kernel_width_en, data_shape[1]), activation='linear', padding='same', kernel_regularizer=regularizer, input_shape=data_shape))\n",
    "  for _ in range(layer_count // 2):\n",
    "    encoder.add(layers.Conv2D(channel_count_en, (kernel_width_en, data_shape[1]), strides = (4, 1), activation=acfun_in, padding='same', kernel_regularizer=regularizer))\n",
    "  if layer_count % 2 == 1:\n",
    "    encoder.add(layers.Conv2D(channel_count_en, (kernel_width_en, data_shape[1]), strides = (8, 1), activation=acfun_in, padding='same', kernel_regularizer=regularizer))\n",
    "  else:\n",
    "    encoder.add(layers.Conv2D(channel_count_en, (kernel_width_en, data_shape[1]), strides = (4, 1), activation=acfun_in, padding='same', kernel_regularizer=regularizer))\n",
    "\n",
    "  encoder.add(layers.Conv2D(4, (kernel_width_en, data_shape[1]), activation = 'linear', padding='same'))\n",
    "  \n",
    "  decoder = keras.Sequential(name = \"decoder\")\n",
    "  if layer_count % 2 == 1:\n",
    "    decoder.add(layers.Conv2DTranspose(channel_count_de, (kernel_width_de, data_shape[1]), strides=(8, 1), activation=acfun_out, padding='same', input_shape=(data_shape[0]//(2 ** (layer_count + 2), data_shape[1], 4))))\n",
    "  else:\n",
    "    decoder.add(layers.Conv2DTranspose(channel_count_de, (kernel_width_de, data_shape[1]), strides=(4, 1), activation=acfun_out, padding='same'))\n",
    "  for _ in range(layer_count // 2):\n",
    "    decoder.add(layers.Conv2DTranspose(channel_count_de, (kernel_width_de, data_shape[1]), strides=(4, 1), activation=acfun_out, padding='same'))\n",
    "  decoder.add(layers.Conv2D(1, (kernel_width_de, data_shape[1]), activation='linear', padding='same'))\n",
    "\n",
    "  return (encoder, decoder)\n",
    "\n",
    "def create_model(data_shape, activation_en='elu', activation_de='elu', optimizer='adam', kernel_width_en=256, kernel_width_de=64, channel_count_en=16, channel_count_de=16, compression_factor=8, loss='mse', regularizer = None):\n",
    "  \"\"\"creates an autoencoder\"\"\"\n",
    "  enc_dec = create_encoder_decoder(data_shape, compression_factor, activation_en, activation_de, optimizer, kernel_width_en, kernel_width_de, channel_count_en, channel_count_de, regularizer)\n",
    "  model = keras.Sequential(name = \"autoencoder\")\n",
    "  model.add(enc_dec[0])\n",
    "  model.add(enc_dec[1])\n",
    "  model.compile(\n",
    "          loss=loss,\n",
    "          optimizer=optimizer,\n",
    "          metrics=[keras.metrics.MeanSquaredError()])\n",
    "  return (model, enc_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MYdB4PtpV8Xc"
   },
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "  \"\"\"normalizes the data and returns the parameters necessary to reconstruct the original\"\"\"\n",
    "  MIN = data.min()\n",
    "  data = data - MIN\n",
    "  DIV = data.max()\n",
    "  data = data / DIV\n",
    "  return (data, MIN, DIV)\n",
    "\n",
    "def denormalize(data_tuple):\n",
    "  \"\"\"reverts the normalization\"\"\"\n",
    "  (data, MIN, DIV) = data_tuple\n",
    "  return data * DIV + MIN\n",
    "\n",
    "def compression_exponent(factor):\n",
    "  \"\"\"returns the smalles n for 2^n > factor\"\"\"\n",
    "  return np.int_(np.ceil(np.log2(np.float64(factor))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9Ue8dXcuAEn"
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_data_for_training(song_data, compression_factor, window_width, stride, batch_size=8):\n",
    "  \"\"\"creates a generator, that returns batches of the training data\n",
    "  \n",
    "     returns the Generator and the number of steps per epoch\n",
    "  \"\"\"\n",
    "  comp_fac = 2 ** (compression_exponent(compression_factor) + 2)\n",
    "  window_width += (comp_fac - (window_width % comp_fac)) % comp_fac\n",
    "  segment_indices = [(i, i + window_width) for i in range(0, max(1, song_data.shape[0] - window_width), stride)]\n",
    "  def generator():\n",
    "    while True:\n",
    "      windows = map(lambda seg: song_data[seg[0] : seg[1]], segment_indices)\n",
    "      windows = map(lambda segment: np.concatenate([segment, np.zeros((window_width - segment.shape[0], segment.shape[1]))]), windows)\n",
    "      while True:\n",
    "        targets = []\n",
    "        for i, w in zip(range(batch_size), windows):\n",
    "          targets.append(w.reshape((1, window_width, song_data.shape[1], 1)))\n",
    "        if len(targets) > 0:\n",
    "          targets = np.concatenate(targets)\n",
    "          yield (targets, targets)\n",
    "        else:\n",
    "          break\n",
    "\n",
    "  return (generator(), int(np.ceil(len(segment_indices) / batch_size))) \n",
    "  \n",
    "def transform_data_for_model(song_data, compression_factor):\n",
    "  \"\"\"transforms data to have a compatible size fopr the model\"\"\"\n",
    "  comp_fac = 2 ** (compression_exponent(compression_factor) + 2)\n",
    "  padding_size = (comp_fac - (song_data.shape[0] % comp_fac)) % comp_fac\n",
    "  nd = np.concatenate([song_data, np.zeros((padding_size, song_data.shape[1]))])\n",
    "  nd = nd.reshape((1, nd.shape[0], nd.shape[1], 1))\n",
    "  return (nd, padding_size)\n",
    "\n",
    "def transform_data_from_model(model_data, padding):\n",
    "  \"\"\"reverts the transform_data_for_model function\"\"\"\n",
    "  data = model_data[0][:model_data.shape[1] - padding]\n",
    "  return data.reshape((data.shape[0], data.shape[1]))\n",
    "\n",
    "def predict(data, model, compression_factor, overlap = 2**16, segment_size = 2**18):\n",
    "  \"\"\"predicts the data the model\"\"\"\n",
    "  data_segments = [data[max(0, i - overlap):min(i + overlap + segment_size, data.shape[0])] for i in range(0, data.shape[0], segment_size)]\n",
    "  prepared_data = [transform_data_for_model(segment, compression_factor) for segment in data_segments]\n",
    "  model_input = [d[0] for d in prepared_data]\n",
    "  raw_prediction = [model.predict(i) for i in model_input]\n",
    "  prediction = [transform_data_from_model(tup[0], tup[1][1]) for tup in zip(raw_prediction, prepared_data)]\n",
    "  padding_free_prediction = [prediction[0][:segment_size]] + [pred[overlap : -overlap] for pred in prediction[1:-1]] + [prediction[-1][overlap:]]\n",
    "  return np.concatenate(padding_free_prediction)\n",
    "\n",
    "def evaluate(data, model, compression_factor, overlap = 2**16, segment_size = 2**18):\n",
    "  \"\"\"evaluates the model based on mse\"\"\"\n",
    "  pred = predict(data, model, compression_factor, overlap, segment_size)\n",
    "  return np.sum((pred - data) ** 2) / data.size\n",
    "\n",
    "def compress(data, compressor, compression_factor, overlap = 2 ** 16, segment_size = 2 ** 18):\n",
    "  \"\"\"Compress the given input data and returns the code\"\"\"\n",
    "  data_segments = [data[max(0, i - overlap):min(i + overlap + segment_size, data.shape[0])] for i in range(0, data.shape[0], segment_size)]\n",
    "  prepared_data = [transform_data_for_model(segment, compression_factor) for segment in data_segments]\n",
    "  preds = [[compressor.predict(i[0]), np.int32(i[1])] for i in prepared_data]\n",
    "  return preds \n",
    "\n",
    "def decompress(data, decompressor, compression_factor, overlap = 2 ** 16, segment_size = 2 ** 18):\n",
    "  \"\"\"decompress the compressed data\"\"\"\n",
    "  model_input = [d[0] for d in data]\n",
    "  raw_prediction = [decompressor.predict(i) for i in model_input]\n",
    "  prediction = [transform_data_from_model(tup[0], tup[1][1]) for tup in zip(raw_prediction, data)]\n",
    "  padding_free_prediction = [prediction[0][:segment_size]] + [pred[overlap : -overlap] for pred in prediction[1:-1]] + [prediction[-1][overlap:]]\n",
    "  return np.concatenate(padding_free_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kMVx35XJOFq1"
   },
   "outputs": [],
   "source": [
    "compression_data = np.load('compressed.npy', allow_pickle=True)\n",
    "MIN = compression_data[1]\n",
    "DIV = compression_data[2]\n",
    "samplerate = compression_data[3]\n",
    "\n",
    "compressed_data = compression_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ut2WBnzPO6kP"
   },
   "outputs": [],
   "source": [
    "compression_factor = 4\n",
    "\n",
    "activation_en = 'selu'\n",
    "activation_de = 'selu'\n",
    "channel_size_en = 16\n",
    "channel_size_de = 16\n",
    "kernel_size_en = 64\n",
    "kernel_size_de = 64\n",
    "optimizer = 'adam'\n",
    "loss = 'mae'\n",
    "\n",
    "\n",
    "(enc_dec) = create_encoder_decoder((None, compressed_data[0][0].shape[2], 1),\n",
    "                            activation_en = activation_en,\n",
    "                            activation_de = activation_de,\n",
    "                            optimizer = optimizer,\n",
    "                            kernel_width_en = kernel_size_en,\n",
    "                            kernel_width_de = kernel_size_de,\n",
    "                            channel_count_en = channel_size_en,\n",
    "                            channel_count_de = channel_size_de,\n",
    "                            compression_factor = compression_factor)\n",
    "\n",
    "decoder = enc_dec[1]\n",
    "decoder.build((1, None, compressed_data[0][0].shape[2], 4))\n",
    "decoder.load_weights('weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cd-3hJmekNmb",
    "outputId": "4544c84c-adb2-4b0c-cbae-7f0d24e3a816"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11648640, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded = decompress(compressed_data, decoder, compression_factor)\n",
    "decoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TVWlYNIIBNpU"
   },
   "outputs": [],
   "source": [
    "sf.write(\"result.flac\", decoded, samplerate)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Decoder.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
